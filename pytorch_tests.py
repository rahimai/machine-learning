# -*- coding: utf-8 -*-
"""hw5_p5_lr2965.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OMCbCAF2RaaO7py031E1D4esSLvlJMFz
"""

import torch
import torch.nn as nn
import torch.optim as optim 
import torch.nn.functional as F
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
import numpy as np

class OneLayerNet(nn.Module):
  def __init__(self):
    super(OneLayerNet, self).__init__()
    self.linear = nn.Linear(2, 1)

  def forward(self, x):
    return self.linear(x)

class TwoLayerNet(nn.Module):
  def __init__(self):
    super(TwoLayerNet, self).__init__()
    self.linear1 = nn.Linear(2,2)
    self.linear2 = nn.Linear(2,1)

  def forward(self, x):
    x = F.relu(self.linear1(x))
    x = self.linear2(x)
    return x 

class ThreeLayerNet(nn.Module):
  def __init__(self):
    super(ThreeLayerNet, self).__init__()
    self.linear1 = nn.Linear(64, 64)
    self.linear2 = nn.Linear(64, 32)
    self.linear3 = nn.Linear(32, 1)

  def forward(self, x):
    x = F.relu(self.linear1(x))
    x = F.relu(self.linear2(x))
    x = self.linear3(x)
    return x 

class ConvNet(nn.Module):
  def __init__(self):
    super(ConvNet, self).__init__()
    self.conv1 = nn.Conv2d(1, 8, 3)
    self.conv2 = nn.Conv2d(8, 4, 3)
    self.linear = nn.Linear(4, 1)

  def forward(self, x):
    x = torch.tensor(x.reshape([-1, 1, 8, 8]), dtype=torch.float)
    x = F.max_pool2d(F.relu(self.conv1(x)), 2)
    x = F.relu(self.conv2(x))
    x = x.view(x.shape[0], -1)
    x = self.linear(x)
    return x 

def XOR_data():
  X = torch.tensor([[-1., -1.], [1., -1.], [-1., 1.], [1., 1.]])
  Y = (torch.prod(X, dim=1) < 0.).float()
  return X, Y.view(-1,1)

def digits_data():
  digits, labels = load_digits(return_X_y=True)
  digits = torch.tensor(digits.reshape([-1, 8, 8]), dtype=torch.float)
  labels = torch.tensor(labels.reshape([-1, 1]) % 2 == 1, dtype=torch.float)
  test_digits = digits[:180,:,:]
  test_labels = labels[:180]
  digits = digits[180:,:,:]
  labels = labels[180:]
  return digits, labels, test_digits, test_labels

def gradient_descent(net, X, Y, num_iterations, eta):
  # Instantiate Stochastic Gradient Descent and reset the gradients
  optimizer = optim.SGD(net.parameters(), lr=eta)
  optimizer.zero_grad()
  # Instantiate the objective function 
  objective_fn = nn.BCEWithLogitsLoss()
  # Start gradient descent 
  loss = objective_fn(net(X), Y)
  # Store the initial objective values and error rates
  with torch.no_grad():
    objective_values = [ loss.item() ]
    error_rates = [ error_rate(net(X), Y).item() ]
  for _ in range(num_iterations):
    # Update parameters and reset grad buffer
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    # Make a prediction and calculate the loss 
    output = net(X)
    loss = objective_fn(output, Y)
    with torch.no_grad():
      objective_values.append(loss.item())
      error_rates.append(error_rate(output, Y).item())
  return objective_values, error_rates

def gradient_descent2(net, X, Y, num_iterations, eta):
  # Instantiate Stochastic Gradient Descent and reset the gradients
  optimizer = optim.SGD(net.parameters(), lr=eta)
  optimizer.zero_grad()
  # Instantiate the objective function 
  objective_fn = nn.BCEWithLogitsLoss()
  # Start gradient descent 
  loss = objective_fn(net(X), Y)
  # Store the initial objective values and error rates
  with torch.no_grad():
    objective_values = [ loss.item() ]
    error_rates = [ error_calc(net(X), Y).item() ]
  for _ in range(num_iterations):
    # Update parameters and reset grad buffer
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    # Make a prediction and calculate the loss 
    output = net(X)
    loss = objective_fn(output, Y)
    with torch.no_grad():
      objective_values.append(loss.item())
      error_rates.append(error_calc(output, Y).item())
  return objective_values, error_rates

def error_rate(Yhat, Y):
  return ((torch.sign(Yhat) > 0).float() != Y).float().mean()

def error_calc(Yhat, Y):
  return ((Yhat > 0.5).float() != Y).float().mean()

if __name__ == '__main__':
  XOR_X, XOR_Y = XOR_data()
  digits, labels, test_digits, test_labels = digits_data()
  digits2 = torch.tensor(digits.reshape([-1, 64]), dtype=torch.float)

  num_iterations = 25
  eta = 1.0

  torch.manual_seed(0)
  net1 = OneLayerNet()
  objective_values, error_rates = gradient_descent(net1, XOR_X, XOR_Y, num_iterations, eta)
  
  x = np.arange(0, 26, 1)
  o = plt.figure(1)
  plt.plot(x, objective_values, color = 'g', marker = '.')
  plt.ylabel('Objective values')
  plt.xlabel('Number of iterations')
  o.show()
  plt.figure(1).savefig('Obj1.png')

  e = plt.figure(2)
  plt.plot(x, error_rates, color = 'g', marker = '.')
  plt.ylabel('Error rates')
  plt.xlabel('Number of iterations')
  plt.figure(2).savefig('Err1.png')
  e.show()

  #############################################################################

  torch.manual_seed(0)
  net2 = TwoLayerNet()
  objective_values, error_rates = gradient_descent(net2, XOR_X, XOR_Y, num_iterations, eta)
  x = np.arange(0, 26, 1)
  o2 = plt.figure(4)
  plt.plot(x, objective_values, color = 'g', marker = '.')
  plt.ylabel('Objective values')
  plt.xlabel('Number of iterations')
  o2.show()
  plt.figure(4).savefig('Obj2.png')

  e2 = plt.figure(5)
  plt.plot(x, error_rates, color = 'g', marker = '.')
  plt.ylabel('Error rates')
  plt.xlabel('Number of iterations')
  e2.show()
  plt.figure(5).savefig('Err2.png')

  #############################################################################


  num_iterations = 500
  eta = 0.1

  torch.manual_seed(0)
  net3 = ThreeLayerNet()
  objective_values, error_rates = gradient_descent2(net3, digits2, labels, num_iterations, eta)
  print("ThreeLayerNet: Test error rate: {0}".format(error_rate(net3(test_digits.view(-1,64)),test_labels)))
  x = np.arange(0, 501, 1)
  o3 = plt.figure(6)
  plt.plot(x, objective_values, color = 'g', marker = '.')
  plt.ylabel('Objective values')
  plt.xlabel('Number of iterations')
  o3.show()
  plt.figure(6).savefig('Obj3.png')

  e3 = plt.figure(7)
  plt.plot(x, error_rates, color = 'g', marker = '.')
  plt.ylabel('Error rates')
  plt.xlabel('Number of iterations')
  e3.show()
  plt.figure(7).savefig('Err3.png')



  torch.manual_seed(0)
  net4 = ConvNet()
  objective_values, error_rates = gradient_descent2(net4, digits, labels, num_iterations, eta)
  x = np.arange(0, 501, 1)
  o4 = plt.figure(8)
  plt.plot(x, objective_values, color = 'g', marker = '.')
  plt.ylabel('Objective values')
  plt.xlabel('Number of iterations')
  o4.show()
  plt.figure(8).savefig('Obj4.png')

  e4 = plt.figure(9)
  plt.plot(x, error_rates, color = 'g', marker = '.')
  plt.ylabel('Error rates')
  plt.xlabel('Number of iterations')
  e4.show()
  plt.figure(9).savefig('Err4.png')
  print("ConvNet: Test error rate: {0}".format(error_rate(net4(test_digits),test_labels)))

